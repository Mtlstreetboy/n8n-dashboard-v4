# ğŸ¤– n8n Local Stack - Agentic AI avec Ollama

Architecture de dÃ©veloppement pour l'automatisation intelligente avec traitement local des LLMs.

## ğŸ“Š Diagrammes d'Architecture

Ouvrez les fichiers `.mmd` avec l'extension VS Code **Markdown Preview Mermaid Support** ou collez-les dans [mermaid.live](https://mermaid.live) pour visualisation interactive.

### Diagrammes disponibles :

1. **`architecture.mmd`** : Vue d'ensemble du systÃ¨me (composants et flux de donnÃ©es)
2. **`workflow-example.mmd`** : SÃ©quence d'exÃ©cution d'un workflow agentique typique
3. **`usage-guide.mmd`** : Guide Ã©tape par Ã©tape pour dÃ©velopper et maintenir

## ğŸš€ DÃ©marrage Rapide

### Lancer la stack
```powershell
docker-compose up -d
```

### AccÃ©der Ã  n8n
- URL : http://localhost:5678
- User : `admin`
- Password : `supersecurepassword`

### TÃ©lÃ©charger un modÃ¨le IA
```powershell
# ModÃ¨le recommandÃ© (8B paramÃ¨tres - Standard)
docker exec -it ollama_local_ai ollama run llama3

# ModÃ¨le plus puissant (70B - NÃ©cessite 32GB+ RAM)
docker exec -it ollama_local_ai ollama run llama3:70b

# ModÃ¨le franÃ§ais optimisÃ©
docker exec -it ollama_local_ai ollama run mistral
```

## ğŸ“ Structure du Projet

```
n8n-local-stack/
â”œâ”€â”€ docker-compose.yml       # DÃ©finition de la stack
â”œâ”€â”€ Dockerfile               # Image n8n custom avec Python
â”œâ”€â”€ .env                     # Secrets (ne pas commiter)
â”œâ”€â”€ local_scripts/           # ğŸ Vos scripts Python (mappÃ©s dans n8n)
â”‚   â””â”€â”€ clean_data.py        # Exemple de traitement de donnÃ©es
â”œâ”€â”€ local_files/             # ğŸ“ Fichiers de donnÃ©es (CSV, JSON)
â”œâ”€â”€ architecture.mmd         # ğŸ“Š Diagramme architecture globale
â”œâ”€â”€ workflow-example.mmd     # ğŸ“Š Diagramme sÃ©quence workflow
â””â”€â”€ usage-guide.mmd          # ğŸ“Š Guide d'utilisation visuel
```

## ğŸ”— Connexions ClÃ©s

### Depuis n8n vers Ollama
- **URL Ã  utiliser** : `http://ollama:11434` âš ï¸ (Pas `localhost`)
- **Raison** : n8n tourne dans un conteneur Docker, il doit utiliser le nom du service

### Depuis votre machine vers n8n
- **URL** : `http://localhost:5678`

### Scripts Python dans n8n
- **Chemin Ã  utiliser** : `/data/scripts/clean_data.py`
- **Commande** : `python3 /data/scripts/clean_data.py`

## ğŸ’¡ Workflow de DÃ©veloppement

1. **Ã‰diter le code Python** dans VS Code (`local_scripts/`)
2. **Sauvegarder** (`Ctrl+S`)
3. **ExÃ©cuter immÃ©diatement** depuis n8n (pas de rebuild nÃ©cessaire)
4. **Debugger** en ajoutant des `print()` dans votre script Python (visible dans les logs n8n)

## ğŸ› ï¸ Commandes Utiles

### Voir les logs en temps rÃ©el
```powershell
# Logs n8n
docker logs -f n8n_data_architect

# Logs Ollama
docker logs -f ollama_local_ai
```

### Lister les modÃ¨les installÃ©s
```powershell
docker exec -it ollama_local_ai ollama list
```

### Tester Ollama manuellement
```powershell
docker exec -it ollama_local_ai ollama run llama3
```
*(Tapez votre question, puis `Ctrl+D` pour sortir)*

### RedÃ©marrer la stack
```powershell
docker-compose restart
```

### ArrÃªter tout
```powershell
docker-compose down
```

## ğŸ“¦ ModÃ¨les IA RecommandÃ©s

| ModÃ¨le | Taille | RAM NÃ©cessaire | Usage |
|--------|--------|----------------|-------|
| `llama3` | 8B | 8 GB | Standard (RecommandÃ©) |
| `mistral` | 7B | 8 GB | FranÃ§ais optimisÃ© |
| `llama3:70b` | 70B | 40 GB | TrÃ¨s haute performance |
| `codellama` | 13B | 16 GB | SpÃ©cialisÃ© code |
| `deepseek-coder` | 33B | 24 GB | Code avancÃ© |

## ğŸ”’ SÃ©curitÃ©

âš ï¸ **Cette configuration est pour le dÃ©veloppement local uniquement.**

Pour la production :
- Changez les mots de passe dans `.env`
- Activez HTTPS
- Configurez un reverse proxy (Traefik/Nginx)
- Ne mappez pas les volumes en lecture/Ã©criture

## ğŸ†˜ DÃ©pannage

### "Cannot connect to Ollama"
- VÃ©rifiez que le conteneur tourne : `docker ps`
- Utilisez `http://ollama:11434` et non `localhost`

### "Model not found"
- TÃ©lÃ©chargez-le : `docker exec -it ollama_local_ai ollama pull llama3`

### Script Python ne se met pas Ã  jour
- VÃ©rifiez que vous Ã©ditez bien `local_scripts/` (pas un autre dossier)
- RedÃ©marrez le workflow dans n8n

### Ollama est lent
- Les gros modÃ¨les (70B+) nÃ©cessitent un GPU NVIDIA avec beaucoup de VRAM
- Utilisez des modÃ¨les plus petits (7B-13B) pour CPU/RAM standard

## ğŸ“š Ressources

- [Documentation n8n](https://docs.n8n.io)
- [Documentation Ollama](https://ollama.ai/library)
- [Mermaid Diagrams](https://mermaid.js.org)

---

**Architecture maintenue par :** Senior Data Architect
**Date :** 2025-11-30