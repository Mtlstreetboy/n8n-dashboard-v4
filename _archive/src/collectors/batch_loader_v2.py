#!/usr/bin/env python3
"""
üöÄ BATCH LOADER V2.0 - Parall√©lisation Intelligente
--------------------------------------------------------------------
Inspir√© du Batch Loader original mais adapt√© pour le stack n8n.

Architecture:
- Google News scraping (gnews lib)
- Yahoo Finance scraping (yfinance lib)
- Parall√©lisation: 3-4 tickers simultan√©s, 8 workers par ticker
- Rate limiting: 0.5s Google, 0.2s Yahoo
- Cache intelligent avec backup

Performance:
- 20 tickers √ó 160 jours = ~15 minutes (vs 45 min s√©quentiel)
- Speedup: 2.5-3x
"""
import sys
sys.path.insert(0, '/data/scripts')

import os
import json
import time
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import traceback

from gnews import GNews
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from companies_config import get_all_companies

# Configuration
NEWS_DATA_DIR = '/data/files/companies'
MAX_PARALLEL_TICKERS = 3  # Tickers simultan√©s (optimal: 3-4)
MAX_WORKERS_PER_TICKER = 8  # Workers pour les jours
DAYS_TO_SCRAPE = 160  # Historique complet
RATE_LIMIT_GOOGLE = 0.5  # Secondes entre requ√™tes Google
RATE_LIMIT_YAHOO = 0.2  # Secondes entre requ√™tes Yahoo (pas utilis√© pour l'instant)

class GlobalRateLimiter:
    """Rate limiter thread-safe partag√© entre tous les workers"""
    
    def __init__(self):
        self.locks = {
            'google': Lock(),
            'yahoo': Lock()
        }
        self.last_request = {
            'google': 0,
            'yahoo': 0
        }
    
    def wait_if_needed(self, source='google'):
        """Attend si n√©cessaire pour respecter le rate limit"""
        with self.locks[source]:
            elapsed = time.time() - self.last_request[source]
            delay = RATE_LIMIT_GOOGLE if source == 'google' else RATE_LIMIT_YAHOO
            
            if elapsed < delay:
                time.sleep(delay - elapsed)
            
            self.last_request[source] = time.time()

# Instance globale du rate limiter
rate_limiter = GlobalRateLimiter()

class HistoricalEngine:
    """
    Moteur de collecte historique avec cache optimis√©
    """
    
    def __init__(self, ticker, company_name, search_terms):
        self.ticker = ticker
        self.company_name = company_name
        self.search_terms = search_terms if isinstance(search_terms, list) else [ticker]
        self.cache_file = os.path.join(NEWS_DATA_DIR, f'{ticker}_news.json')
        self.cache = self._load_cache()
        self.vader = SentimentIntensityAnalyzer()
    
    def _load_cache(self):
        """Charge le cache existant avec gestion d'erreur"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Format attendu: {"ticker": "X", "articles": [...]}
                if isinstance(data, dict) and 'articles' in data:
                    print(f"   üìÇ Cache charg√©: {len(data['articles'])} articles")
                    return data
                elif isinstance(data, list):
                    # Migration format legacy
                    print(f"   üìÇ Cache legacy migr√©: {len(data)} articles")
                    return {'ticker': self.ticker, 'articles': data}
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erreur cache, backup cr√©√©: {e}")
                # Backup avant r√©init
                backup_file = self.cache_file.replace('.json', '_backup.json')
                if os.path.exists(self.cache_file):
                    import shutil
                    shutil.copy(self.cache_file, backup_file)
        
        return {'ticker': self.ticker, 'articles': []}
    
    def _save_cache(self):
        """Sauvegarde le cache avec backup automatique"""
        os.makedirs(NEWS_DATA_DIR, exist_ok=True)
        
        # Backup si fichier existe
        if os.path.exists(self.cache_file):
            backup_file = self.cache_file.replace('.json', '_backup.json')
            import shutil
            shutil.copy(self.cache_file, backup_file)
        
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, indent=2, ensure_ascii=False)
    
    def _fetch_single_day(self, date_obj):
        """
        Fetch articles pour un jour sp√©cifique
        Rate-limited par le GlobalRateLimiter
        """
        try:
            # Rate limiting
            rate_limiter.wait_if_needed('google')
            
            # GNews configuration
            gnews = GNews(
                language='en',
                country='US',
                max_results=20,
                period=None  # On sp√©cifie les dates manuellement
            )
            
            # Recherche pour chaque terme
            articles = []
            for term in self.search_terms:
                try:
                    # GNews ne supporte pas les dates pr√©cises, on filtre apr√®s
                    results = gnews.get_news(term)
                    
                    for item in results:
                        try:
                            # Parse date
                            pub_date_str = item.get('published date', '')
                            # Format GNews: "Tue, 10 Dec 2024 12:00:00 GMT"
                            pub_date = datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %Z')
                            if pub_date.tzinfo is None:
                                pub_date = pub_date.replace(tzinfo=timezone.utc)
                            
                            # Filtrer pour le jour cible (¬±1 jour de marge)
                            days_diff = abs((pub_date.date() - date_obj.date()).days)
                            if days_diff <= 1:
                                # Sentiment VADER
                                text = (item.get('title', '') + ' ' + item.get('description', ''))
                                vader_scores = self.vader.polarity_scores(text)
                                
                                article = {
                                    'title': item.get('title', ''),
                                    'url': item.get('url', ''),
                                    'published_at': pub_date.isoformat(),
                                    'content': item.get('description', ''),
                                    'source': item.get('publisher', {}).get('title', 'Unknown'),
                                    'ticker': self.ticker,
                                    'company_name': self.company_name,
                                    'collected_at': datetime.now(timezone.utc).isoformat(),
                                    'sentiment': {
                                        'vader_compound': vader_scores['compound'],
                                        'vader_pos': vader_scores['pos'],
                                        'vader_neg': vader_scores['neg'],
                                        'vader_neu': vader_scores['neu']
                                    }
                                }
                                articles.append(article)
                        except Exception as e:
                            continue
                
                except Exception as e:
                    print(f"      ‚ö†Ô∏è Erreur terme '{term}': {e}")
                    continue
            
            return {
                'date': date_obj.strftime('%Y-%m-%d'),
                'articles': articles,
                'success': True
            }
        
        except Exception as e:
            return {
                'date': date_obj.strftime('%Y-%m-%d'),
                'articles': [],
                'success': False,
                'error': str(e)
            }
    
    def run_time_machine(self, days=160):
        """
        Collecte historique parall√©lis√©e avec cache intelligent
        """
        print(f"\n   üïê Lancement Time Machine: {days} jours")
        
        # Identifier les jours d√©j√† en cache
        existing_urls = {a['url'] for a in self.cache['articles'] if 'url' in a}
        print(f"   üìÇ Articles en cache: {len(existing_urls)}")
        
        # G√©n√©rer liste des dates √† fetcher
        dates_to_fetch = []
        for i in range(days):
            date_obj = datetime.now(timezone.utc) - timedelta(days=i)
            dates_to_fetch.append(date_obj)
        
        print(f"   üìÖ P√©riodes √† scanner: {len(dates_to_fetch)} jours")
        
        # Parall√©lisation des jours
        new_articles = []
        errors = 0
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS_PER_TICKER) as executor:
            futures = {executor.submit(self._fetch_single_day, date): date for date in dates_to_fetch}
            
            completed = 0
            for future in as_completed(futures):
                completed += 1
                
                try:
                    result = future.result()
                    
                    if result['success']:
                        # Filtrer doublons
                        for article in result['articles']:
                            if article['url'] not in existing_urls:
                                new_articles.append(article)
                                existing_urls.add(article['url'])
                    else:
                        errors += 1
                    
                    # Progress
                    if completed % 20 == 0:
                        print(f"      ‚è≥ {completed}/{len(dates_to_fetch)} jours scann√©s...")
                
                except Exception as e:
                    errors += 1
        
        print(f"   ‚úÖ Scan termin√©: {len(new_articles)} nouveaux articles")
        
        # Merge avec cache
        self.cache['articles'].extend(new_articles)
        
        # D√©dupliquer par URL (s√©curit√©)
        seen_urls = set()
        unique_articles = []
        for article in self.cache['articles']:
            url = article.get('url', '')
            if url and url not in seen_urls:
                unique_articles.append(article)
                seen_urls.add(url)
        
        self.cache['articles'] = unique_articles
        
        # Sauvegarder
        self._save_cache()
        
        # M√©triques qualit√©
        total_articles = len(self.cache['articles'])
        coverage = len([a for a in new_articles if a]) / max(len(dates_to_fetch), 1) * 100
        avg_per_day = total_articles / max(days, 1)
        
        return {
            'ticker': self.ticker,
            'success': True,
            'existing': total_articles - len(new_articles),
            'new': len(new_articles),
            'total': total_articles,
            'errors': errors,
            'coverage': coverage,
            'avg_articles_per_day': avg_per_day,
            'confidence_score': min((coverage + avg_per_day * 10) / 2, 100)
        }

def process_single_ticker(company, days=160):
    """
    Traite un ticker complet (wrapper pour parall√©lisation)
    """
    ticker = company['ticker']
    company_name = company['name']
    search_terms = company.get('search_terms', [ticker])
    
    start_time = time.time()
    
    try:
        print(f"\n{'='*80}")
        print(f"üéØ {ticker} - {company_name}")
        print(f"{'='*80}")
        
        engine = HistoricalEngine(ticker, company_name, search_terms)
        result = engine.run_time_machine(days)
        
        elapsed = time.time() - start_time
        result['duration'] = elapsed
        
        print(f"\n‚úÖ {ticker} - SUCCESS")
        print(f"   ‚è±Ô∏è Termin√© en {elapsed:.1f}s")
        print(f"   üìä Qualit√©: {result['confidence_score']:.1f}/100")
        print(f"   üì∞ Articles/jour: {result['avg_articles_per_day']:.1f}")
        print(f"   üÜï Nouveaux: {result['new']}")
        print(f"   üìö Total: {result['total']}")
        
        return result
    
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"\n‚ùå {ticker} - FAILED")
        print(f"   Error: {str(e)}")
        traceback.print_exc()
        
        return {
            'ticker': ticker,
            'success': False,
            'error': str(e),
            'duration': elapsed
        }

def batch_load_parallel(days=160):
    """
    Charge tous les tickers en parall√®le
    """
    companies = get_all_companies()
    
    print("\n" + "="*80)
    print("üöÄ BATCH LOADER V2.0 - PARALL√âLISATION INTELLIGENTE")
    print("="*80)
    print(f"üìä Tickers: {len(companies)}")
    print(f"üìÖ Historique: {days} jours")
    print(f"‚ö° Parall√©lisation: {MAX_PARALLEL_TICKERS} tickers simultan√©s")
    print(f"üîß Workers/ticker: {MAX_WORKERS_PER_TICKER}")
    print(f"‚è±Ô∏è Dur√©e estim√©e: ~{len(companies) * 60 / MAX_PARALLEL_TICKERS / 60:.0f} minutes")
    print("="*80)
    
    start_time = time.time()
    results = []
    
    # Parall√©lisation des tickers
    with ThreadPoolExecutor(max_workers=MAX_PARALLEL_TICKERS) as executor:
        futures = {executor.submit(process_single_ticker, company, days): company for company in companies}
        
        completed = 0
        for future in as_completed(futures):
            completed += 1
            company = futures[future]
            
            try:
                result = future.result()
                results.append(result)
                
                print(f"\n[{completed}/{len(companies)}] ‚úÖ {company['ticker']} termin√©")
            
            except Exception as e:
                print(f"\n[{completed}/{len(companies)}] ‚ùå {company['ticker']} √©chou√©: {e}")
                results.append({
                    'ticker': company['ticker'],
                    'success': False,
                    'error': str(e)
                })
    
    # Rapport final
    elapsed = (time.time() - start_time) / 60
    
    successes = [r for r in results if r.get('success')]
    failures = [r for r in results if not r.get('success')]
    
    total_new = sum(r.get('new', 0) for r in successes)
    total_articles = sum(r.get('total', 0) for r in successes)
    avg_quality = sum(r.get('confidence_score', 0) for r in successes) / max(len(successes), 1)
    
    print("\n" + "="*80)
    print("üéâ BATCH LOADER TERMIN√â")
    print("="*80)
    print(f"‚úÖ Succ√®s: {len(successes)}/{len(companies)}")
    print(f"‚ùå √âchecs: {len(failures)}")
    print(f"üÜï Nouveaux articles: {total_new}")
    print(f"üìö Total articles: {total_articles}")
    print(f"üìä Qualit√© moyenne: {avg_quality:.1f}/100")
    print(f"‚è±Ô∏è Dur√©e: {elapsed:.1f} minutes")
    print(f"‚ö° Speedup: ~{len(companies) * 3 / elapsed:.1f}x vs s√©quentiel")
    print("="*80)
    
    # D√©tails
    print("\nüìä D√âTAIL PAR TICKER:")
    print("-"*80)
    print(f"{'Ticker':<10} {'Status':<10} {'Nouveaux':<10} {'Total':<10} {'Qualit√©':<10}")
    print("-"*80)
    
    for r in sorted(results, key=lambda x: x.get('new', 0), reverse=True):
        status = '‚úÖ OK' if r.get('success') else '‚ùå FAIL'
        new = r.get('new', 0)
        total = r.get('total', 0)
        quality = f"{r.get('confidence_score', 0):.0f}/100" if r.get('success') else 'N/A'
        print(f"{r['ticker']:<10} {status:<10} {new:<10} {total:<10} {quality:<10}")
    
    print("-"*80)
    
    if failures:
        print("\n‚ùå √âCHECS:")
        for r in failures:
            print(f"   {r['ticker']}: {r.get('error', 'Unknown error')}")

if __name__ == "__main__":
    days = DAYS_TO_SCRAPE
    
    if len(sys.argv) > 1:
        try:
            days = int(sys.argv[1])
        except:
            pass
    
    # Installation des d√©pendances si n√©cessaire
    try:
        import gnews
        import vaderSentiment
    except ImportError:
        print("üì¶ Installation des d√©pendances...")
        os.system("pip install gnews vaderSentiment -q")
    
    batch_load_parallel(days=days)
